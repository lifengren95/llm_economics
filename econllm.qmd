---
title: "Introduction to Large Language Models (LLM) and Economics"
author: "Lifeng Ren"
date: "11-29-2023"
format: 
    html: 
        toc: true
        toc-depth: 3
        number-sections: true
        mathjax: local
        highlight: tango
    pdf:
        toc: true
        number-sections: true
        colorlinks: true
python: python3
---

Today, we are going to talk about the Large Language Models (LLM) and how it can be used in Economics Research. 

![Big Data and Large Language Models Cover Figure generated by `DALL-E 2`](./imgs/DALLE_cover.png){fig-align="center" width=60%}

# Lecture Agenda

- LLM Basics

- GPT API Demo

- `nanoGPT` Demo

- `linktransformer` Demo

- `GeoLLM` Paper

- Other Economics Research Applications


# LLM Basics

## Transformer, GPT, Bert

- The following three are the papers for the transformer(2017), GPT(Jun.2018), and Bert (Oct.2018)
  - [Attention is all you need](https://arxiv.org/abs/1706.03762)
  - [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
  - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

The main architecture figure below is from the `Attention` paper and it shows the architecture of the transformer. The transformer is the basic architecture of the GPT and Bert.

![Source: Niklas Heidloff: "Foundation Models, Transformers, BERT and GPT"](./imgs/transformers.png){fig-align="center" width=60%}

You can see a more detailed explanation of the transformer from the following picture:

![Source: Ali Issa: "Transformer, GPT-3,GPT-J, T5 and BERT."](./imgs/transformer_explain.png){fig-align="center" width=60%}

## Attention Mechanism

The attention mechanism is the key to the transformer. It is the mechanism that allows the transformer to learn the context of the words.

**Example of Attention:**

- Consider the sentence: "*The cat sat on the mat.*"

If a neural network model is tasked to translate this sentence into another language, it might use attention to focus on different words at different times. For instance:

- When translating "The cat," the model might pay more attention to "cat," understanding it's the subject of the sentence.

- As it moves to "sat on," the model's focus shifts to understanding the action - "sat" and its relation with "cat."

- Finally, for "the mat," the attention mechanism helps the model link "mat" as the object where the action is taking place.

- Throughout this process, the attention mechanism dynamically adjusts which words (or parts of words) in the input sentence are most relevant at each step, allowing the model to create a coherent and contextually accurate translation.

**Multi-Head Attention:**

You can see this example from Jay Alammar's blog post: [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

When processing the word "it," one attention head primarily concentrates on "the animal," while another gives more weight to "tired." Essentially, the model's interpretation of "it" incorporates elements from the representations of both "animal" and "tired."

![](./imgs/multihead_attention.png){fig-align="center" width=60%}

## BERT

I will use this following example to explain the BERT model:

Original Unlabeled Text:

"John picked up his `MASK` to call his mother."

Converted Text with Masked Token:

"John picked up his `phone` to call his mother."

Explanation:

In this example, BERT is given a sentence with a key word masked out - represented by "MASK". The model's task is to predict the masked word based on the context provided by the rest of the sentence.

In this case, the context clues - "picked up" and "to call his mother" - suggest that the masked word is likely an object used for communication, leading BERT to predict "phone."

You can also see the example from the original paper:

![Source: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"](./imgs/bert_example.png){fig-align="center" width=60%}

Note on Self-Supervised Learning:

This process, where BERT generates labeled data from originally unlabeled text by predicting masked words, is a form of self-supervised learning. It's effective for processing large datasets, as BERT can learn rich representations of language by understanding the context and relationships between words without needing explicit human-labeled training data.

## GPT

GPT is decoder only transformer. GPT knows the previous, it predicts the future. (In economics, it is the $\hat{Y}$)

- From the `GPT` Paper: Given an unsupervised corpus of tokens $\mathcal{U}=\left\{u_1, \ldots, u_n\right\}$, we use a standard language modeling objective to maximize the following likelihood:
    $$
    L_1(\mathcal{U})=\sum_i \log P\left(u_i \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)
    $$

And this is the very simple architecture of the GPT:

![Source: "Language Models: GPT and GPT-2
How smaller language models inspired modern breakthroughs" by Cameron R. Wolfe](./imgs/GPT_simpleexample.png){fig-align="center" width=80%}



## Train my own GPT model?

Here is a flow created by Dr.Andrej Karpathy in his [Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g?si=XfGIKNX7E7t6Nh6w) video: 

![Source: Andrej Karpathy: "Intro to Large Language Models"](./imgs/trainyourownai.png){fig-align="center" width=60%}


---

**So, basically, it is not possible to train our own GPT-3 Level model. But we have other options**

- Use the GPT API

- Load Local LLM Models
    - Today, we are going to use `nanoGPT` to train a GPT model on our own machine

    - In the future, you can use the `huggingface` to load the open source LLM models, such as `LLAMA2`. 

---



# OpenAI API

Since not all of you have access to the OpenAI API, I will go through the API with you and show you the basic usage of the API by using it to standardize the addresses. You can also use the API to other insteresting things, such as:

- Web Scraping: GPT-4 Vision API + Puppeteer
    - <https://www.youtube.com/watch?v=VeQR17k7fiU>





## GPT API to standardize addresses




```{python}
#| eval: false
import openai
from openai import OpenAI
import pandas as pd
import os

# Set your OpenAI API key
api_key = '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'
os.environ["OPENAI_API_KEY"] = api_key
client = OpenAI()


def clean_address(address):
    """
    Send an address to the GPT API for cleaning and standardization.

    Parameters:
    address (str): The address to be cleaned.

    Returns:
    str: The cleaned and standardized address.
    """

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are an address expert that familiar with all the USPS address standard."}, 
            {"role": "user", "content": f"Clean and standardize this address. I only need the new address. Do not explain.: {address}"}],
        max_tokens=60
    )
    return response.choices[0].message.content

# Load the CSV file with the addresses
df = pd.read_csv('./unstandardized_addresses.csv')  # Adjust the path to your CSV file

# Cleaning each address in the DataFrame
df['cleaned_address'] = df['address'].apply(clean_address)

# Save the cleaned data to a new CSV
df.to_csv('./cleaned_location_data.csv', index=False)  # Adjust the path as needed

```

- The `unstandardized_addresses.csv` is also generated by `ChatGPT`.

## Redo The Sentiment Analysis using GPT API, compare with VADER
```{python}
#| eval: false
import openai
from openai import OpenAI
import pandas as pd
import os

# Set your OpenAI API key
api_key = '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'
os.environ["OPENAI_API_KEY"] = api_key
client = OpenAI()

test_sentences = [
    "This new song is lit üî•üî•üî•",
    "Sigh... I guess today was just not my day üòû",
    "Wow, that's awesome!!! üòÉüëç",
    "I can't stand this! So frustrating! üò°",
    "IDK what's going on, kinda confused rn ü§∑‚Äç‚ôÇÔ∏è",
    "LOL, that was hilarious üòÇ",
    "Ugh, Mondays are the worst üò´",
    "OMG, I just got the job offer!!! üòç",
    "No way, that's cray cray üòú",
    "Why is everyone so glum? Cheer up! üòä"
]

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "Assume the role of a sentiment analysis specialist. Your task is to evaluate the sentiment of given sentences, categorizing them as positive, neutral, or negative. Each sentence should be scored with a fraction representing its sentiment in each category. Ensure that the combined total of these three fractional scores equals 1 for each sentence, with each score ranging from 0 to 1. Provide a balanced and precise sentiment analysis, reflecting the nuanced emotional content of each statement."}, 
        {"role": "user", "content": f"{test_sentences}"}],
)


print(response.choices[0].message)


```

Output:

```{python}
#| eval: false
ChatCompletionMessage(content='[\'This new song is lit üî•üî•üî•\']: \nPositive: 0.9\nNeutral: 0.1\nNegative: 0.0\n\n[\'Sigh... I guess today was just not my day üòû\']: \nPositive: 0.1\nNeutral: 0.2\nNegative: 0.7\n\n["Wow, that\'s awesome!!! üòÉüëç"]: \nPositive: 0.9\nNeutral: 0.1\nNegative: 0.0\n\n["I can\'t stand this! So frustrating! üò°"]: \nPositive: 0.1\nNeutral: 0.2\nNegative: 0.7\n\n["IDK what\'s going on, kinda confused rn ü§∑\\u200d‚ôÇÔ∏è"]: \nPositive: 0.1\nNeutral: 0.8\nNegative: 0.1\n\n[\'LOL, that was hilarious üòÇ\']: \nPositive: 0.9\nNeutral: 0.1\nNegative: 0.0\n\n[\'Ugh, Mondays are the worst üò´\']: \nPositive: 0.1\nNeutral: 0.3\nNegative: 0.6\n\n[\'OMG, I just got the job offer!!! üòç\']: \nPositive: 0.9\nNeutral: 0.1\nNegative: 0.0\n\n["No way, that\'s cray cray üòú"]: \nPositive: 0.8\nNeutral: 0.2\nNegative: 0.0\n\n[\'Why is everyone so glum? Cheer up! üòä\']: \nPositive: 0.9\nNeutral: 0.1\nNegative: 0.0', role='assistant', function_call=None, tool_calls=None)
```

As we recalled from the Lexical/Dictonary Based Sentiment Analysis, the VADER is not doing a good job on the test case we provided as we can see below.

![VADER's results](./imgs/VADER_demo.png){fig-align="center" width=100%}

Here is the data formatted into a Markdown table. The table includes two sets of sentiment analysis results for each sentence: one from your ChatCompletionMessage and another from VADER, excluding the compound score:

| Sentence | ChatCompletion Positive | ChatCompletion Neutral | ChatCompletion Negative | VADER Positive | VADER Neutral | VADER Negative |
|----------|-------------------------|------------------------|-------------------------|----------------|---------------|----------------|
| "This new song is lit üî•üî•üî•" | 0.9 | 0.1 | 0.0 | 0.0 | 0.41 | 0.59 |
| "Sigh... I guess today was just not my day üòû" | 0.1 | 0.2 | 0.7 | 0.289 | 0.711 | 0.0 |
| "Wow, that's awesome!!! üòÉüëç" | 0.9 | 0.1 | 0.0 | 0.617 | 0.383 | 0.0 |
| "I can't stand this! So frustrating! üò°" | 0.1 | 0.2 | 0.7 | 0.0 | 0.618 | 0.382 |
| "IDK what's going on, kinda confused rn ü§∑‚Äç‚ôÇÔ∏è" | 0.1 | 0.8 | 0.1 | 0.0 | 0.685 | 0.315 |
| "LOL, that was hilarious üòÇ" | 0.9 | 0.1 | 0.0 | 0.593 | 0.295 | 0.112 |
| "Ugh, Mondays are the worst üò´" | 0.1 | 0.3 | 0.6 | 0.0 | 0.29 | 0.71 |
| "OMG, I just got the job offer!!! üòç" | 0.9 | 0.1 | 0.0 | 0.279 | 0.721 | 0.0 |
| "No way, that's cray cray üòú" | 0.8 | 0.2 | 0.0 | 0.0 | 0.784 | 0.216 |
| "Why is everyone so glum? Cheer up! üòä" | 0.9 | 0.1 | 0.0 | 0.451 | 0.366 | 0.183 |


---

**Beyond using the API, can we load our 

---

# `nanoGPT` Do it Together

- We are going to go through one of the most hands on tutorial on a transformer model: `nanoGPT` developed by Andrej Karpathy
- Please go to the repositary and `git clone` the `nanoGPT` to your local machine
    - <https://github.com/karpathy/nanoGPT>
- I will go through the code with you and explain the details.

## Install necessary packages

- Open your `miniforge3` terminal (WIN) or `terminal` (MAC)

```{python}
#| eval: false
mamba init
mamba activate 8222env3

pip install torch numpy transformers datasets tiktoken wandb tqdm
```

## Relocate your folder to the `nanoGPT` folder

```{python}
#| eval: false

pwd # check your current working directory

cd /Users/lifengren/github/nanoGPT # change the directory to the nanoGPT folder


```

## Run `prepare.py` in the `shakespeare_char` folder

You can do this either using 

```{python}
#| eval: false

python data/shakespeare_char/prepare.py
```


- This will automatically generate 4 files for you:
  - `input.txt`
  - `meta.pkl`
  - `train.bin`
  - `val.bin`


## Run `train_shakespeare_char.py` in the `config` folder

Now we can use the given input to train our model. Since we are using the CPU, we can use the following command:

```{python}
#| eval: false

python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
```


## Run `sample.py` in the `shakespeare_char` folder

Now, we can use the trained model to generate some text in shakespeare style. We can use the following command:

`python sample.py --out_dir=out-shakespeare-char --device=cpu`

## You just load a very simple GPT model on your own machine!

You can try this on Co-Lab + Lambda GPU and you can actually train a GPT-2 model!

For more details on how to train a GPT-2 model, and fine tune it, please refer to the following tutorial:

- <https://github.com/karpathy/nanoGPT>


# Economics Research Applications

## `linktransformer`

This is one example of applciations of LLM in Economics Research. You can use for the data cleaning and data linking section. I provided a python script for you to try out the `linktransformer` package.

- For more information, please refer to their GitHub page: <https://github.com/dell-research-harvard/linktransformer>

```{python}
#| eval: false
from sentence_transformers import SentenceTransformer
import linktransformer as lt
import pandas as pd
# Example usage of lm_merge_df

data2 = {
    "CompanyName": ["TechCorp", "InfoTech Solutions", "GlobalSoft Inc", "DataTech Co", "SoftSys Ltd", "TechCorp"],
    "Industry": ["Technology", "Technology", "Software", "Data Analytics", "Software", "Technology"],
    "Founded_Year": [2005, 1998, 2010, 2012, 2003, 2005]
}

# Create a DataFrame from the data
df2 = pd.DataFrame(data2)

data1 = {
    "CompanyName": ["Tech Corporation", "InfoTech Soln", "GlobalSoft Incorporated", "DataTech Corporation", 
                    "SoftSys Limited", "TechCorp", "AlphaSoft Systems"],
    "Revenue (Millions USD)": [5000, 4500, 3000, 2500, 4000, 5500, 3800],
    "Num_Employees": [10000, 8500, 6000, 5000, 7500, 12000, 7000],
    "Country": ["USA", "Canada", "India", "Germany", "UK", "USA", "Spain"]
}

# Create a DataFrame from the data
df1 = pd.DataFrame(data1)

df_lm_matched = lt.merge(df1, df2, merge_type='1:m', on="CompanyName", model="all-MiniLM-L6-v2",
left_on=None, right_on=None)

print(df_lm_matched)

```

---

In addition to some basic data analysis skills that can be obviously done using the LLM, there are some other interesting applications of LLM in Economics Research. 

---


## `GeoLLM`

This is one of paper that using LLM's knowledge to do some spatial-soci-economic analysis data prediction.

![GeoLLM Paper](./imgs/geollm.png){fig-align="center" width=100%}

- The study demonstrates that LLMs contain substantial spatial information about various locations. 

- They introduce GeoLLM, a novel method that effectively extracts geospatial knowledge from LLMs using auxiliary map data from OpenStreetMap. 

- The utility of GeoLLM is demonstrated across various crucial tasks, such as measuring population density and economic livelihoods.

## Let us try together/demo with different prompts on gpt3.5 and gpt4

![Source: GeoLLM Paper-Fig1](./imgs/geollm_trytogether.png){fig-align="center" width=100%}


# Where to find more resources

- Hugging Face

![](./imgs/huggingface.png){fig-align="center" width=100%}

- GitHub
    - <https://github.com/topics/large-language-models>
    - <https://github.com/topics/chatgpt>
    - <https://github.com/f/awesome-chatgpt-prompts>
    - <https://github.com/openai/openai-cookbook>

- YouTube


